---
typora-copy-images-to: ./
---

# PointNet

## 论文

第一个将点云直接用于深度学习

2017 

CVPR

[CVPR 2017 Open Access Repository (thecvf.com)](https://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html)

### 摘要

点云是一种重要的几何数据结构。由于其无序性，大部分研究把这种数据转换成了有序的3D体素网格或者图片集合。然而，这会使数据不必要地大量增加，并导致问题。在本文中，我们设计了一种直接处理点云的新型神经网络，这种网络很好的考虑了输入中的点的有序性。PointNet，这种神经网络为物体分类、部分分割和场景语义分析提供了统一的结构。虽然简单，但PointNet十分高效且有效。根据经验，这种神经网络表现出了和现有技术水平相当甚至更好的强劲表现。从理论上，我们为解释这种网络学习到了什么和为什么这种网络能够对输入数据的扰动和损坏保持高度鲁棒性进行了分析。

## Ⅰ 介绍

本文我们探索了能够处理3D几何数据（eg.点云、网格）的深度学习模型。典型的卷积结构要求高度规则的输入数据格式（eg.图片网格、3D体素数据），这是为了实现权重分享和其他核函数优化。由于点云和网格都不是有序的格式，大部分研究通常在把这些数据转换成有序的3D体素网格或者图片集合（eg.视图）后，再把数据喂给深度网络模型。然而，这种数据表示转换会使结果数据不必要地大量增加，同时也会引入能模糊数据自然不变性的量化伪像。

因此，我们使用简单的点云专注3D几何不同的输入表示，并命名生成的深度网络为PointNets。点云是简单且统一的结构，能够避免网格的组合不规则性和复杂性，因此更容易学习。然而，PointNet仍然需要重视这样一个事实，点云只是一组点，因此对其成员的排列不变性需要在网络计算中进行某些对称化操作。而更进一步的刚性运动的不变性也需要考虑。

PointNet是一个统一的模型，直接将点云作为输入，同时输出整个输入的类标签或者输入的的每个点块部分的标签。该模型的基础构架非常的简单，因为在初始阶段，每个点的处理方法完全相同且独立。在基础设置中，每个点仅用它的三个坐标![1666187489850](.\1666187489850.png)来表示。额外维度可以通过计算法线和其他局部或者全局特征来添加得到。

我们这个方法的关键在于使用单个对称函数，最大池。网络有效地学习一组优化函数/准则，选择点云中感兴趣的或者含有信息的点并编码其选择的原因。网络最终的全连接层把这些学习到的最优值聚合到如上所述的整个形状的全局描述符（形状分类）上或者用于预测每个点标签（形状分割）。

我们的输入格式很容易应用刚性或者仿射变换，因为每个点都是独立变换的。因此我们能够增加一个依赖数据的空间变换网络，该网络会在PointNet处理数据前对数据进行规范化，以便进一步改善结果。

我们提供了该方法的理论分析和实验评估。我们展示了我们的网络可以近似任何连续的集合函数。更有趣的是，事实证明我们的网络通过学习一组稀疏的关键点来概括一个输入点云，这些关键点根据可视化大致对应物体对象的骨架。理论分析对为什么PointNet对输入点的小的扰动以及通过插入（异常点）或删除（错误点）造成的损坏具有高度鲁棒性提供了解释。

在形状分类、部分分割、场景分割的许多基准数据集中，我们通过实验将PointNet和基于多视图和体积表示的最先进方法进行了对比。在统一的构架下，PointNet不仅在速度上更快，同时也表现出了和现有技术相当甚至更好的性能。

我们工作的主要贡献如下：

+ 我们设计了一个适用于处理3D无序点集的新型深度网络模型
+ 我们展示了如何训练这样的网络模型来执行3D形状分类、形状部分分割和场景语义分析任务
+ 我们对该方法的稳定性和效率进行了深入的验证和理论分析
+ 我们演示了网络中所选择神经元计算出的3D特征，并对其性能进行了直观的解释

通过神经网络处理无序集合的问题是一个非常普遍和根本的问题，我们希望我们的想法也可以应用到其他领域。

![1666187568351](.\1666187568351.png)

> PointNet 的应用
>
> 我们提出了一种新型深度网络模型，该模型可以处理原始点云（点集）而无需体素化或者渲染。它是一个统一的构架，能够学习全局和局部点特征，维许多3D识别任务提供一个简单高效且有效的方法。

## Ⅱ 相关工作

### 点云特征 

点云大部分的现有特征都是针对特定任务手工制作的。点的特征通常编码点的某些统计特性，并且被设计成对于某些变换是不变的，这些变换通常被划分为内在的或外在的。它们还可以归类为局部特征和全局特征。对于特定任务，找到最优的特征组合并非易事。

### 3D数据的深度学习 

3D数据具有多种流行的表示形式，从而有各种学习方法。Volumetric CNNs: 是在体素形状上应用3D卷积神经网络的先驱。然而，由于数据的稀疏性和3D卷积的计算成本，体积表示受到其分辨率的限制。FPNN 和 Vote3D提出了处理稀疏性问题的特殊方法；然而，他们的操作仍然是在稀疏的体积上，处理非常大的点云对他们来说是一种挑战。Multiview CNNs: 试图将3D点云或形状渲染成2D图像，然后应用2D卷积神经网络对它们进行分类。通过精心设计的图像CNN，这一系列方法在形状分类和检索任务方面取得了主导作用。然而，将它们扩展到场景理解或其他3D任务（eg.点分类、形状完成）上时表现很普通。Spectral CNNs: 最近的一些工作在网格上使用了光谱CNN。然而，这些方法目前受限于流形网格（eg.有机物体），而且如何将它们扩展到非等距形状（eg.家具）上并不明显。Feature-based DNNs: 首先通过提取传统的形状特征将3D数据转换成向量，然后使用全连接网络对形状进行分类。我们认为它们受限于所提取特征的表示能力。

### 无序集的深度学习 

从数据结构的角度来说，点云是无序的向量集合。当大部分深度学习工作集中在规则输入表示，如序列（eg.语音和语言处理）、图像和体积（eg.视频、3D数据）上时，很少有在点集上做深度学习工作的。

Oriol Vinyals等人最近的一项工作研究了这个问题，他们使用具有注意机制的读-处理-写网络来处理无序输入集，同时展示他们的网络具有对数字进行排序的能力。然而，由于他们的工作重点是泛型集合NLP应用，因此缺少了几何体在集合中的作用。

## Ⅲ 问题陈述

我们设计了一个深度学习框架，直接使用无序点集作为输入。点云表示为一组3D点![1666187946774](.\1666187946774.png)其中每个点的![1666187985897](.\1666187985897.png)是其坐标![1666188012212](.\1666188012212.png)向量加上额外的特征通道例如颜色、法线等，除非另有说明，我们只使用坐标![1666188012212](.\1666188012212.png)作为我们的点的通道。

对于对象分类任务，输入点云要么直接从形状采样，要么从场景点云中预分割。我们提出的深度网络输出所有k kk个候选类别的k kk分数。对于语义分割，输入可以是用于部分区域分割的单个对象，或者是用于对象区域分割的3D场景的一个子体积。我们的模型将为 n 个点和 m 个语义子分类中的每一个输出 n × m 分数。

## Ⅳ 点集的深度学习

我们网络的构架的灵感来自 ![1666188155404](.\1666188155404.png) 中的点集的属性。

### ![1666188166023](.\1666188166023.png)中点集的属性

我们的输入是来自欧式空间的点的子集。它有三个主要属性：

+ 无序性 与图像中的像素数组或体积网格中的体素数组不同，点云是一组没有特定顺序的点。换句话说，处理 ![1666188215172](.\1666188215172.png) 个3D点集的网络需要能够对输入数据集按照喂进来的顺序排列 ![1666188222154](.\1666188222154.png) 保持不变。
+ 点之间的相互作用 这些点来自具有距离度量的空间。这意味着这些点不是孤立的，而且相邻的点形成一个具有意义的子集。因此模型需要能够捕获附近点的局部结构，以及局部结构之间组合的相互作用。
+ 转换的不变性 对于几何对象，点集学习到的表示应该对某些变换具有不变性。例如，所有的旋转点和平移点都不能修改全局点云的类别或者点的分割。

### PointNet的结构

我们整个网络的结构都在图2中展示出来了，其中分类网络和分割网络占据了结构中的很大一部分。请阅读图2流程图的说明文字。

![1666188299932](.\1666188299932.png)

分类网络将 N 个点作为输入，作用输入和特征变换，然后通过最大池聚合点的特征。输出的是 k 种分类的分类分数。分割网络是分类网络的拓展，它连接局部和全局特征以及每个点输出的分数。“mlp”代表多层感知机，括号中的数字是层的大小。Batchnorm用于ReLU的所有层。Dropout层用于分类网络中的最后一个多层感知机。
我们的网络有三个关键模块：

- 最大池层作为对称函数用于聚类所有点的信息
- 组合局部信息和全局信息的结构
- 两个对齐输入点和点的特征的联合对齐网络

我们将在下面的单独段落中讨论这些设计选择背后的原因。

#### 无序输入的对称函数

为了使模型对输入排序不变，存在三种策略：1）将输入排序为规范顺序；2）将输入看作训练RNN的序列，但其通过各种排列来增加训练数据；3）用一个简单的对称函数来聚类每个点的信息。这里，对称函数将 n 个向量作为输入，同时输出一个对输入顺序不变的新向量。例如，+ 和 ∗ 操作符是对称二元函数。

尽管排序听起来像是一个简单的解决方法，但谈及到一般意义上的点的扰动时，在高维空间中实际上并不存在稳定的排序。这可以很容易地通过矛盾来显示。如果这样一种排序策略存在，那么它定义了高维空间和 1d 实线之间的双向映射。不难看出，谈及点扰动时要求排序的稳定等同在维度降低时保持映射在空间上的接近度，这在一般情况下是无法实现的任务。因此，排序并不能完全解决排序问题，而且因为排序问题的存在，网络很难学习到一致的从输入到输出的映射。如实验（图5）中所示，我们发现直接在排序点集上应用多层感知机变现不好，但稍微比直接处理无序输入要好。

使用RNN的想法是将点集作为顺序信号，并希望用随机置换序列训练RNN，这样RNN将会变得和输入顺序无关。然而，在“OrderMatters”中，作者已经证明顺序确实很重要而且无法完全被忽略。尽管RNN对于具有小长度（数十个）的序列的输入排序已经具有相对良好的鲁棒性，但是很难扩展到成千上万的输入元素上，而这是点集的很常见的规模。根据实验，我们也证明了基于RNN的模型表现的并不如我们所提出来的方法（图5）。

我们的想法是通过对集合中转换元素应用对称函数来近似在点集上定义的一般函数：

![1666188371880](.\1666188371880.png)

根据实验，我们的基础模型十分简单：我们通过多层感知机来近似h hh，并通过单个变量函数和最大池函数的组合来近似 g 。通过实验发现这表现的很好。通过h hh的集合,我们可以学到一些 f 来捕获点集的不同属性。

尽管我们的关键模块看起来很简单，但它也具有很精彩的地方（参见5.3节），在一些不同的应用中能够表现出很强的性能（参见5.1节）。由于我们模型的简单性，我们也提供了4.3节中的理论分析。

#### 局部和全局信息聚类

上一节的输出组成了一个向量![1666188420809](.\1666188420809.png)，这是输入集的全局标签。我们可以在形同全局特征上很容易训练SVM或多层感知机分类器以进行分类。然而，点的分割需要局部和全局新的的结合。我们能够通过简单而高效的方式来实现这一目标。

我们的解决方法可以在图2（Segmentation Network）中看到。在计算完全局点云特征向量后，我们将全局特征和每一个点的特征连接起来反馈给每一个点特征。然后我们基于组合的点特征提取新的每个点的特征，这样每个点特征都考虑了局部和全局信息。

通过这样的修改，我们的网络能够预测依赖于局部几何和全局语义的每个点数量。例如我们可以准确地预测出每个点的法线（图中的补充），验证网络能够汇总来自该点的局部领域的信息。在实验环节中，我们也表明我们的模型可以在形状部分分割和场景分割方面实现最先进的性能。

#### 联合对准网络

如果点云在经历了某些几何变换（eg刚性变换），那么点云的语义标签必须是不变的。因此我们希望点集的学习表示对这些变换是不变的。

一个自然的解决方法是在特征提取前将所有的输入集对齐到规范空间。Jaderberg等人[9]介绍了一种通过采样和插值来对齐2D图像的空间变换的想法，通过在GPU上特别定制的层来实现。

与[9]相比，我们的点云输入形式使我们能够以更简单的方式来实现这一目标。我们不需要发明任何新的层，也不需要像图像情况那样引入任何别名。我们通过mini-network（图2中的T-net）预测仿射变换矩阵，并直接将该变换作用于输入点的坐标。Mini-network本身类似于大型网络，且由点的独立特征提取、max pooling和全连接层组成。更多关于T-net的细节在补充中。

这个想法可以进一步扩展到特征空间的对齐。我们可以在点的特征上插入另一个对齐网络，并预测特征变换矩阵以对齐来自不同输入点云的特征。然而，特征空间中的变换矩阵比空间变换矩阵的维度高很多，这极大地增加了优化的难度。因此，我们在softmax的训练损失中增加了一个正则化项。我们约束特征变换矩阵接近于正交矩阵。

![1666188455817](.\1666188455817.png)

其中 A 是mini-network预测的特征对齐矩阵。正交变换不会丢失输入的信息，因此期望是正交变换。我们发现通过增加正则化项，优化变得更加稳定，而且我们的模型实现了更好的性能。

### 理论分析

#### 通用逼近

首先，我们展示了我们的神经网络对于连续集函数的通用逼近能力。通过集函数的连续性，直观地，对输入点集的小扰动不应该极大地改变函数值，例如分类或分割的分数。

形式上，让![1666188559286](.\1666188559286.png)是涉及Hausdorff距离![1666188579788](.\1666188579788.png)上的连续集合函数，换句话说，∀ ϵ &gt; 0 ， ∃ δ &gt; 0 对于任意![1666188605591](.\1666188605591.png),如果![1666188638242](.\1666188638242.png)那么c![1666188659458](.\1666188659458.png)。定理的意思是说，在max pooling层给定足够多的神经元时，f 可以由我们的网络任意近似，换句话说，（1）中的K 足够大。
![1666188696282](.\1666188696282.png)

在我们的补充材料中给出了这个定理的证明。关键的思路是，在最坏的情况下，模型可以通过将空间划分成等大小的体素来学习将点云转换成体积表示。然而，实验中模型学习到了一种更加智能的探测空间的策略，正如我们将在点函数可视化中看到的那样。

#### 维度和稳定性的瓶颈 

 从理论上和实验中我们发现，模型的表现受到max pooling层维度的很大影响，即（1）中的 K 。这里我们提供了一个分析，揭示了和模型的稳定性相关的属性。

我们定义 f 的子网络为![1666188748421](.\1666188748421.png),其将![1666188760598](.\1666188760598.png)中点集映射到 K 维向量以下定理告诉我们输入集中的小损失或者额外噪声点都不太可能改变模型的输出。

![1666188787677](.\1666188787677.png)

结合 h 的连续性，这解释了模型在涉及点扰动、损失和额外噪声点情况下的鲁棒性。类似于机器学习模型中的稀疏性原则，可以得到鲁棒性。直观上，我们的模型通过学习一组稀疏的关键点来总结一个形状。在实验中我们可以看到关键点构成了对象的骨架。

## Ⅴ 实验











